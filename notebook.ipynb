{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "from logger import utils\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pydub import AudioSegment\n",
    "from ddsp.vocoder import F0_Extractor, Volume_Extractor, Units_Encoder\n",
    "from logger.utils import traverse_dir\n",
    "from preprocess import preprocess\n",
    "from train import ddsp_train\n",
    "from draw import main\n",
    "from sep_wav import demucs, audio_norm, get_ffmpeg_args\n",
    "from main import inference\n",
    "# Cuda setting\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# configure loading\n",
    "args = utils.load_config('./configs/sins.yaml')\n",
    "\n",
    "# set path\n",
    "MP4_DATA_PATH   = 'preprocess/mp4'\n",
    "ORIGINAL_PATH   = 'preprocess/original/'\n",
    "DEMUCS_PATH     = 'preprocess/demucs/'\n",
    "NORM_PATH       = 'preprocess/norm/'\n",
    "TEMP_LOG_PATH   = 'temp_ffmpeg_log.txt'  # ffmpeg의 무음 감지 로그의 임시 저장 위치"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터 전처리\n",
    "***\n",
    "1. 난 전처리가 필요없다. 배경음 제거가 완벽하고, 모든 데이터들도 특정 길이로 잘 잘려져있다.\n",
    "    - 데이터를 전부 data/train/audio/안에 다 집어 넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        data/train/audio/aaa.wav\n",
    "        data/train/audio/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.6 validation 분리단계로 점프\n",
    "***\n",
    "2. 난 거의 다 되어 있지만 데이터가 너무 길다. 특정 길이로 자르고 싶다.\n",
    "    - 데이터를 전부 preprocess/split 안에 다 집어 넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/norm/aaa.wav\n",
    "        preprocess/norm/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.4 split 단계로 점프\n",
    "***\n",
    "3. 난 배경음도 제거해야되고 데이터도 길다. 거의 날 것의 상태다.\n",
    "    - 데이터를 전부 preprocess/original 안에 다 집어넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/original/aaa.wav\n",
    "        preprocess/original/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.2부터 demucs 단계로 점프\n",
    "***\n",
    "4. 난 아무것도 안되어 있고, 심지어 mp4파일이다.\n",
    "    - 데이터들 전부 preprocess/mp4에 다 집어넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/mp4/aaa.mp4\n",
    "        preprocess/mp4/bbb.mp4\n",
    "        ...\n",
    "        ```\n",
    "    - 1.1부터 차례대로 진행"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1데이터가 mp4인 경우 (wav만 있는 경우에는 패스)\n",
    "preprocess/mp4 안에 있는 mp4파일을 wav로 변경 해서 preprocess/original 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "def mp4_to_wav(input_dir : str, input_file: str, output_dir: str):\n",
    "    \"\"\"mp4파일을 wav형식으로 변환합니다.\n",
    "    Args:\n",
    "        input_dir (str) : 입력 mp4파일의 path\n",
    "        input_file (str) : 입력 mp4파일의 이름\n",
    "        output_dir (str) : 출력 wav파일의 path\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(input_file)[1][1:]\n",
    "\n",
    "    if ext != \"mp4\":\n",
    "        return \n",
    "    else :\n",
    "        track = AudioSegment.from_file(os.path.join(input_dir,input_file),  format= 'mp4')\n",
    "        track = track.set_frame_rate(44100)\n",
    "        track.export(os.path.join(output_dir, input_file[:-4]+\".wav\"), format='wav')\n",
    "\n",
    "\n",
    "filelist =  traverse_dir(\n",
    "    MP4_DATA_PATH,\n",
    "    extension='mp4',\n",
    "    is_pure=True,\n",
    "    is_sort=True,\n",
    "    is_ext=True)\n",
    "\n",
    "for file in tqdm(filelist):\n",
    "    mp4_to_wav(MP4_DATA_PATH, file, ORIGINAL_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 무음제거 (Demucs)\n",
    "preprocess/original에 있는 wav파일들의 음악소리를 제거하고 목소리만 추출해서 preprocess/demucs 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 03:28:16 | INFO | torchaudio.utils.download | The local file (C:\\Users\\wlsdm\\.cache\\torch\\hub\\torchaudio\\models\\hdemucs_high_trained.pt) exists. Skipping the download.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "목소리 추출 중...: 100%|██████████| 12/12 [00:14<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "demucs(ORIGINAL_PATH, DEMUCS_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 normalize\n",
    "preprocess/demucs에 있는 배경음이 제거된 데이터들을 노멀라이즈 해서 preprocess/norm에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "노멀라이징 작업 중...: 100%|██████████| 12/12 [00:07<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "for filepath in tqdm(glob(DEMUCS_PATH+\"*.wav\"), desc=\"노멀라이징 작업 중...\"):\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    out_filepath = os.path.join(NORM_PATH, filename) + \".wav\"\n",
    "    audio_norm(filepath, out_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 split\n",
    "preprocess/norm에 있는 노말라이즈된 데이터들을 15초 길이로 잘라서 data/train/audio에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "음원 자르는 중...: 100%|██████████| 12/12 [00:00<00:00, 14.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for filepath in tqdm(glob(NORM_PATH+\"*.wav\"), desc=\"음원 자르는 중...\"):\n",
    "    duration = librosa.get_duration(filename=filepath)\n",
    "    max_last_seg_duration = 0\n",
    "    sep_duration_final = 15\n",
    "    sep_duration = 15\n",
    "\n",
    "    while sep_duration > 4:\n",
    "        last_seg_duration = duration % sep_duration\n",
    "        if max_last_seg_duration < last_seg_duration:\n",
    "            max_last_seg_duration = last_seg_duration\n",
    "            sep_duration_final = sep_duration\n",
    "        sep_duration -= 1\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    out_filepath = os.path.join(args.data.train_path,\"audio\", f\"{filename}-%04d.wav\")\n",
    "    subprocess.run(f'ffmpeg -i \"{filepath}\" -f segment -segment_time {sep_duration_final} \"{out_filepath}\" -y', capture_output=True, shell=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 무음 제거\n",
    "data/train/audio에 있는 잘라진 음원들 중에 무음인 파일들을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "무음 제거 중...: 100%|██████████| 108/108 [00:04<00:00, 23.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for filepath in tqdm(glob(args.data.train_path+\"/audio/*.wav\"), desc=\"무음 제거 중...\"):\n",
    "    if os.path.exists(TEMP_LOG_PATH):\n",
    "        os.remove(TEMP_LOG_PATH)\n",
    "\n",
    "    ffmpeg_arg = get_ffmpeg_args(filepath)\n",
    "    subprocess.run(ffmpeg_arg, capture_output=True, shell=True)\n",
    "\n",
    "    start = None\n",
    "    end = None\n",
    "\n",
    "    with open(TEMP_LOG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if \"lavfi.silence_start\" in line:\n",
    "                start = float(line.split(\"=\")[1])\n",
    "            if \"lavfi.silence_end\" in line:\n",
    "                end = float(line.split(\"=\")[1])\n",
    "\n",
    "    if start != None:\n",
    "        if start == 0 and end == None:\n",
    "            os.remove(filepath)\n",
    "            \n",
    "if os.path.exists(TEMP_LOG_PATH):\n",
    "        os.remove(TEMP_LOG_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 학습 데이터 중, 일부를 validaion용으로 자동으로 보내준다.\n",
    "- data/train/audio에 있는 데이터 중 일정 비율만큼 알아서 data/val/audio로 이동시켜준다\n",
    "    - 계산식은 다음과 같다 `max(2, min(10, 전체 데이터 * 0.01))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1342.39it/s]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 (학습에 쓰기 위한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Encoder Model] HuBERT Soft\n",
      " [Loading] pretrain/hubert/hubert-soft-0d54a1f4.pt\n",
      "Preprocess the audio clips in : data/train\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:07<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess the audio clips in : data/val\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 18.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "sample_rate = args.data.sampling_rate\n",
    "hop_size = args.data.block_size\n",
    "\n",
    "# initialize f0 extractor\n",
    "f0_extractor = F0_Extractor(\n",
    "                    args.data.f0_extractor, \n",
    "                    args.data.sampling_rate, \n",
    "                    args.data.block_size, \n",
    "                    args.data.f0_min, \n",
    "                    args.data.f0_max)\n",
    "\n",
    "# initialize volume extractor\n",
    "volume_extractor = Volume_Extractor(args.data.block_size)\n",
    "                    \n",
    "# initialize units encoder\n",
    "units_encoder = Units_Encoder(\n",
    "                    args.data.encoder, \n",
    "                    args.data.encoder_ckpt, \n",
    "                    args.data.encoder_sample_rate, \n",
    "                    args.data.encoder_hop_size, \n",
    "                    device = device)    \n",
    "\n",
    "# preprocess training set\n",
    "preprocess(args.data.train_path, f0_extractor, volume_extractor, units_encoder, sample_rate, hop_size, device = device)\n",
    "\n",
    "# preprocess validation set\n",
    "preprocess(args.data.valid_path, f0_extractor, volume_extractor, units_encoder, sample_rate, hop_size, device = device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >    exp: exp/sins-test\n",
      " [DDSP Model] Sinusoids Additive Synthesiser\n",
      "Load all the data from : data/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:00<00:00, 108.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load all the data from : data/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 100.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- model size ---\n",
      "model: 3,375,360\n",
      "======= start training =======\n",
      "epoch: 1 |   4/  5 | exp/sins-test | batch/s: 1.61 | loss: 5.305 | time: 0:00:06.2 | step: 10\n",
      "epoch: 3 |   4/  5 | exp/sins-test | batch/s: 7.76 | loss: 4.875 | time: 0:00:07.5 | step: 20\n",
      "epoch: 5 |   4/  5 | exp/sins-test | batch/s: 7.49 | loss: nan | time: 0:00:08.8 | step: 30\n",
      "epoch: 7 |   4/  5 | exp/sins-test | batch/s: 7.60 | loss: nan | time: 0:00:10.1 | step: 40\n",
      "epoch: 9 |   4/  5 | exp/sins-test | batch/s: 7.30 | loss: nan | time: 0:00:11.5 | step: 50\n",
      "epoch: 11 |   4/  5 | exp/sins-test | batch/s: 8.15 | loss: nan | time: 0:00:12.7 | step: 60\n",
      "epoch: 13 |   4/  5 | exp/sins-test | batch/s: 7.45 | loss: nan | time: 0:00:14.0 | step: 70\n",
      "epoch: 15 |   4/  5 | exp/sins-test | batch/s: 7.53 | loss: nan | time: 0:00:15.4 | step: 80\n",
      "epoch: 17 |   4/  5 | exp/sins-test | batch/s: 7.46 | loss: nan | time: 0:00:16.7 | step: 90\n",
      "epoch: 19 |   4/  5 | exp/sins-test | batch/s: 7.84 | loss: nan | time: 0:00:18.0 | step: 100\n",
      "epoch: 21 |   4/  5 | exp/sins-test | batch/s: 7.74 | loss: nan | time: 0:00:19.3 | step: 110\n",
      "epoch: 23 |   4/  5 | exp/sins-test | batch/s: 7.47 | loss: nan | time: 0:00:20.6 | step: 120\n",
      "epoch: 25 |   4/  5 | exp/sins-test | batch/s: 7.47 | loss: nan | time: 0:00:22.0 | step: 130\n",
      "epoch: 27 |   4/  5 | exp/sins-test | batch/s: 7.99 | loss: nan | time: 0:00:23.2 | step: 140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ddsp_train(args)\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\project\\DDSP-SVC-KOR\\train.py:156\u001b[0m, in \u001b[0;36mddsp_train\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    153\u001b[0m loader_train, loader_valid \u001b[39m=\u001b[39m get_data_loaders(args, whole_audio\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    155\u001b[0m \u001b[39m# run\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m train(args, initial_global_step, model, optimizer, loss_func, loader_train, loader_valid)\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\project\\DDSP-SVC-KOR\\solver.py:102\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, initial_global_step, model, optimizer, loss_func, loader_train, loader_test)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# backpropagate\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 102\u001b[0m     scaler\u001b[39m.\u001b[39;49mstep(optimizer)\n\u001b[0;32m    103\u001b[0m     scaler\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m    105\u001b[0m \u001b[39m# log loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:370\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    368\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 370\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    372\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[0;32m    374\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:289\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    288\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39;49m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m optimizer_state[\u001b[39m\"\u001b[39;49m\u001b[39mfound_inf_per_device\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues()):\n\u001b[0;32m    290\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:289\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    288\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    290\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddsp_train(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결과물 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [DDSP Model] Combtooth Subtractive Synthesiser\n",
      " [Loading] exp/combsub-test/model_best.pt\n",
      "MD5: 44aa2cc3a0a8aa04bde53b9e9b7e729c\n",
      "Loading pitch curves for input audio from cache directory...\n",
      "Extracting the volume envelope of the input audio...\n",
      " [Encoder Model] HuBERT Soft\n",
      " [Loading] pretrain/hubert/hubert-soft-0d54a1f4.pt\n",
      "Enhancer type: nsf-hifigan\n",
      "| Load HifiGAN:  pretrain/nsf_hifigan/model\n",
      "Removing weight norm...\n",
      "Speaker ID: 1\n",
      "Cut the input audio into 3 slices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  9.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# configure setting\n",
    "configures = {\n",
    "    'model_path'            :   'exp/combsub-test/model_best.pt', # 추론에 사용하고자 하는 모델, 바로위에서 학습한 모델을 가져오면댐\n",
    "    'input'                 :   'data/train/audio/video-0000.wav', # 추론하고자 하는 노래파일의 위치 - 님들이 바꿔야댐 \n",
    "    'output'                :   'output.wav',  # 결과물 파일의 위치\n",
    "    'spk_id'                :   '1', \n",
    "    'spk_mix_dict'          :   'None', \n",
    "    'key'                   :   '0', \n",
    "    'enhance'               :   'true' , \n",
    "    'pitch_extractor'       :   'crepe' ,\n",
    "    'f0_min'                :   '50' ,\n",
    "    'f0_max'                :   '1100',\n",
    "    'threhold'              :   '-60',\n",
    "    'enhancer_adaptive_key' :   '0'\n",
    "}\n",
    "cmd = SimpleNamespace(**configures)\n",
    "\n",
    "inference(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8a643f5fe528358e1cfac3836870fd104c9c787e6f994a831162d9d1f5f0281"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
